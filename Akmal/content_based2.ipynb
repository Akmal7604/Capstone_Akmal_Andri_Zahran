{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f5d14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import (\n",
    "    MultiLabelBinarizer, \n",
    "    LabelEncoder, \n",
    "    StandardScaler\n",
    ")\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error  # Tambahkan baris ini\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c55fa538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\\\UB\\\\Dicoding\\\\GitHub\\\\Capstone_Akmal_Andri_Zahran\\\\recipes_crop.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f4d5bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 141788 entries, 0 to 141787\n",
      "Data columns (total 27 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   RecipeId                    141788 non-null  int64  \n",
      " 1   Name                        141788 non-null  object \n",
      " 2   AuthorId                    141788 non-null  int64  \n",
      " 3   AuthorName                  141788 non-null  object \n",
      " 4   CookTime                    141788 non-null  object \n",
      " 5   PrepTime                    141788 non-null  object \n",
      " 6   TotalTime                   141788 non-null  object \n",
      " 7   DatePublished               141788 non-null  object \n",
      " 8   Description                 141788 non-null  object \n",
      " 9   Images                      141788 non-null  object \n",
      " 10  RecipeCategory              141788 non-null  object \n",
      " 11  Keywords                    141788 non-null  object \n",
      " 12  RecipeIngredientQuantities  141788 non-null  object \n",
      " 13  RecipeIngredientParts       141788 non-null  object \n",
      " 14  AggregatedRating            141788 non-null  float64\n",
      " 15  ReviewCount                 141788 non-null  float64\n",
      " 16  Calories                    141788 non-null  float64\n",
      " 17  FatContent                  141788 non-null  float64\n",
      " 18  SaturatedFatContent         141788 non-null  float64\n",
      " 19  CholesterolContent          141788 non-null  float64\n",
      " 20  SodiumContent               141788 non-null  float64\n",
      " 21  CarbohydrateContent         141788 non-null  float64\n",
      " 22  FiberContent                141788 non-null  float64\n",
      " 23  SugarContent                141788 non-null  float64\n",
      " 24  ProteinContent              141788 non-null  float64\n",
      " 25  RecipeServings              141788 non-null  float64\n",
      " 26  RecipeInstructions          141788 non-null  object \n",
      "dtypes: float64(12), int64(2), object(13)\n",
      "memory usage: 29.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5ec16b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUISINE_KEYWORDS = {\n",
    "    'Asian': [\n",
    "        'asian', 'chinese', 'japanese', 'korean', 'thai', 'vietnamese', \n",
    "        'indonesian', 'malaysian', 'filipino', 'indian', 'cambodian', \n",
    "        'nepalese', 'pakistani', 'hunan', 'szechuan', 'cantonese'\n",
    "    ],\n",
    "    'Western': [\n",
    "        'american', 'canadian', 'australian', 'new zealand', 'mexican', \n",
    "        'brazilian', 'chilean', 'colombian', 'peruvian', 'tex mex', \n",
    "        'southwestern u.s.', 'pennsylvania dutch'\n",
    "    ],\n",
    "    'European': [\n",
    "        'italian', 'spanish', 'french', 'german', 'polish', 'russian', \n",
    "        'hungarian', 'czech', 'swedish', 'danish', 'norwegian', 'finnish', \n",
    "        'scottish', 'welsh', 'belgian', 'dutch', 'greek', 'austrian', \n",
    "        'swiss', 'portuguese', 'scandinavian'\n",
    "    ],\n",
    "    'Middle Eastern': [\n",
    "        'lebanese', 'turkish', 'palestinian', 'iraqi', 'egyptian', \n",
    "        'moroccan', 'southwest asia', 'israeli'\n",
    "    ],\n",
    "    'African': [\n",
    "        'south african', 'ethiopian', 'nigerian', 'sudanese', 'somalian', \n",
    "        'african', 'egyptian', 'moroccan'\n",
    "    ],\n",
    "    'Latin American': [\n",
    "        'brazilian', 'chilean', 'colombian', 'costa rican', 'cuban', \n",
    "        'ecuadorean', 'guatemalan', 'honduran', 'mexican', 'peruvian', \n",
    "        'puerto rican', 'venezuelan'\n",
    "    ]\n",
    "}\n",
    "\n",
    "MEAL_TYPE_KEYWORDS = {\n",
    "    'Breakfast': [\n",
    "        'breakfast', 'pancake', 'oatmeal', 'eggs', 'eggs breakfast', \n",
    "        'brunch', 'quick breads', 'yeast breads'\n",
    "    ],\n",
    "    'Lunch': [\n",
    "        'lunch', 'salad', 'sandwich', 'lunch/snacks', 'light meal', \n",
    "        'college food', 'one dish meal'\n",
    "    ],\n",
    "    'Dinner': [\n",
    "        'dinner', 'main course', 'protein', 'meat', 'stew', 'roast', \n",
    "        'poultry', 'whole chicken', 'whole turkey', 'whole duck'\n",
    "    ],\n",
    "    'Dessert': [\n",
    "        'dessert', 'desserts easy', 'sweet', 'cake', 'cookie', 'pie', \n",
    "        'ice cream', 'cheesecake', 'frozen desserts', 'chocolate chip cookies'\n",
    "    ],\n",
    "    'Snack': [\n",
    "        'snack', 'appetizer', 'bar cookie', 'candy', 'quick bite', \n",
    "        'spreads', 'shakes', 'smoothies'\n",
    "    ]\n",
    "}\n",
    "\n",
    "DIET_KEYWORDS = {\n",
    "    'Vegetarian': [\n",
    "        'vegetable', 'vegan', 'egg free', 'dairy free', 'soy/tofu', \n",
    "        'tempeh', 'no meat'\n",
    "    ],\n",
    "    'Pescatarian': [\n",
    "        'fish', 'seafood', 'salmon', 'tuna', 'tilapia', 'halibut', \n",
    "        'mahi mahi', 'catfish', 'trout', 'bass', 'crab', 'lobster', \n",
    "        'shrimp', 'oysters', 'mussels'\n",
    "    ],\n",
    "    'Low Calorie': [\n",
    "        'low protein', 'low cholesterol', 'healthy', 'high fiber', \n",
    "        'very low carbs', 'light meal'\n",
    "    ],\n",
    "    'High Protein': [\n",
    "        'high protein', 'protein', 'chicken breast', 'lean meat', \n",
    "        'turkey breast'\n",
    "    ],\n",
    "    'Special Diets': [\n",
    "        'gluten free', 'lactose free', 'kosher', 'halal', \n",
    "        'no shell fish', 'no cook'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fe22c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_recipe_data(df):\n",
    "    # Handle Missing Values\n",
    "    df.dropna(subset=['Name', 'Calories', 'ProteinContent'], inplace=True)\n",
    "    \n",
    "    # Fungsi ekstraksi fitur dengan multi-label\n",
    "    def extract_multi_label_features(keywords, feature_dict):\n",
    "        keywords_lower = str(keywords).lower()\n",
    "        matched_features = []\n",
    "        \n",
    "        for category, category_keywords in feature_dict.items():\n",
    "            if any(kw in keywords_lower for kw in category_keywords):\n",
    "                matched_features.append(category)\n",
    "        \n",
    "        return matched_features if matched_features else ['Other']\n",
    "    \n",
    "    # Fungsi untuk memastikan single label\n",
    "    def get_single_label(features):\n",
    "        return features[0] if features else 'Other'\n",
    "    \n",
    "    # Ekstraksi fitur\n",
    "    df['Cuisine'] = df['Keywords'].apply(\n",
    "        lambda x: get_single_label(extract_multi_label_features(x, CUISINE_KEYWORDS))\n",
    "    )\n",
    "    \n",
    "    df['MealType'] = df.apply(\n",
    "        lambda row: get_single_label(\n",
    "            extract_multi_label_features(\n",
    "                str(row['Name']) + ' ' + str(row['Keywords']), \n",
    "                MEAL_TYPE_KEYWORDS\n",
    "            )\n",
    "        ), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    df['DietType'] = df['Keywords'].apply(\n",
    "        lambda x: extract_multi_label_features(x, DIET_KEYWORDS)\n",
    "    )\n",
    "    \n",
    "    # Feature Engineering\n",
    "    numeric_features = ['Calories', 'ProteinContent', 'CarbohydrateContent', 'FatContent']\n",
    "    for feature in numeric_features:\n",
    "        df[f'{feature}_Normalized'] = (df[feature] - df[feature].mean()) / df[feature].std()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4074c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_data(processed_df):\n",
    "    # Multi-Label Encoding untuk fitur kategorik\n",
    "    mlb_diet = MultiLabelBinarizer()\n",
    "    diet_encoded = mlb_diet.fit_transform(processed_df['DietType'])\n",
    "    diet_columns = [f'Diet_{col}' for col in mlb_diet.classes_]\n",
    "    diet_df = pd.DataFrame(diet_encoded, columns=diet_columns)\n",
    "\n",
    "    # Label Encoding untuk Cuisine dan MealType\n",
    "    le_cuisine = LabelEncoder()\n",
    "    le_meal_type = LabelEncoder()\n",
    "    \n",
    "    processed_df['Cuisine_Encoded'] = le_cuisine.fit_transform(processed_df['Cuisine'])\n",
    "    processed_df['MealType_Encoded'] = le_meal_type.fit_transform(processed_df['MealType'])\n",
    "\n",
    "    # Gabungkan fitur\n",
    "    X = pd.concat([\n",
    "        processed_df[[\n",
    "            'Calories_Normalized', \n",
    "            'ProteinContent_Normalized', \n",
    "            'CarbohydrateContent_Normalized', \n",
    "            'FatContent_Normalized', \n",
    "            'Cuisine_Encoded', \n",
    "            'MealType_Encoded'\n",
    "        ]],\n",
    "        diet_df\n",
    "    ], axis=1)\n",
    "\n",
    "    # Target Variable\n",
    "    y = processed_df['AggregatedRating']\n",
    "\n",
    "    # Split Data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Scaling tambahan untuk fitur input\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return {\n",
    "        'X_train': X_train_scaled,\n",
    "        'X_test': X_test_scaled,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'cuisine_encoder': le_cuisine,\n",
    "        'meal_type_encoder': le_meal_type,\n",
    "        'diet_encoder': mlb_diet,\n",
    "        'scaler': scaler,\n",
    "        'feature_names': X.columns.tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfe8ed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rating_prediction_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_rating_prediction_model(model_data):\n",
    "    X_train = model_data['X_train']\n",
    "    X_test = model_data['X_test']\n",
    "    y_train = model_data['y_train']\n",
    "    y_test = model_data['y_test']\n",
    "\n",
    "    model = build_rating_prediction_model(X_train.shape[1])\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=15, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train, \n",
    "        validation_split=0.2,  \n",
    "        epochs=10, \n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(f'Test MAE: {mae}')\n",
    "    print(f'Test MSE: {mse}')\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ca2be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_cuisine_preference(cuisine_preference, available_cuisines):\n",
    "    matched_cuisines = []\n",
    "    preference_lower = cuisine_preference.lower()\n",
    "\n",
    "    all_cuisine_keywords = {\n",
    "        keyword.lower(): category \n",
    "        for category, keywords in CUISINE_KEYWORDS.items() \n",
    "        for keyword in keywords\n",
    "    }\n",
    "\n",
    "    if cuisine_preference.lower() in [cat.lower() for cat in CUISINE_KEYWORDS.keys()]:\n",
    "        return [cuisine_preference]\n",
    "    \n",
    "    if preference_lower in all_cuisine_keywords:\n",
    "        return [all_cuisine_keywords[preference_lower]]\n",
    "    \n",
    "    for keyword, category in all_cuisine_keywords.items():\n",
    "        if keyword in preference_lower:\n",
    "            matched_cuisines.append(category)\n",
    "    \n",
    "    return list(set(matched_cuisines)) if matched_cuisines else list(available_cuisines)\n",
    "\n",
    "def recommend_recipe_comprehensive(user_profile, processed_df, model_data, rating_prediction_model):\n",
    "    def calculate_bmi(height, weight):\n",
    "        height_m = height / 100\n",
    "        bmi = weight / (height_m ** 2)\n",
    "        return round(bmi, 2)\n",
    "    \n",
    "    def categorize_bmi(bmi):\n",
    "        if bmi < 18.5: return 'Underweight'\n",
    "        elif 18.5 <= bmi < 25: return 'Normal'\n",
    "        elif 25 <= bmi < 30: return 'Overweight'\n",
    "        else: return 'Obese'\n",
    "    \n",
    "    def calculate_daily_calories(age, gender, weight, height, activity_level):\n",
    "        if gender.lower() == 'male':\n",
    "            bmr = 88.362 + (13.397 * weight) + (4.799 * height) - (5.677 * age)\n",
    "        else:\n",
    "            bmr = 447.593 + (9.247 * weight) + (3.098 * height) - (4.330 * age)\n",
    "        \n",
    "        activity_factors = {\n",
    "            'sedentary': 1.2,\n",
    "            'light': 1.375,\n",
    "            'moderate': 1.55,\n",
    "            'active': 1.725,\n",
    "            'very active': 1.9\n",
    "        }\n",
    "        \n",
    "        daily_calories = bmr * activity_factors.get(activity_level.lower(), 1.2)\n",
    "        return round(daily_calories)\n",
    "\n",
    "    def prepare_recipe_features(recipes, model_data):\n",
    "        numeric_features = recipes[[\n",
    "            'Calories_Normalized', \n",
    "            'ProteinContent_Normalized', \n",
    "            'CarbohydrateContent_Normalized', \n",
    "            'FatContent_Normalized'\n",
    "        ]]\n",
    "\n",
    "        cuisine_encoded = model_data['cuisine_encoder'].transform(recipes['Cuisine'])\n",
    "        meal_type_encoded = model_data['meal_type_encoder'].transform(recipes['MealType'])\n",
    "        diet_encoded = model_data['diet_encoder'].transform(recipes['DietType'])\n",
    "        \n",
    "        X = np.column_stack([\n",
    "            numeric_features,\n",
    "            cuisine_encoded, \n",
    "            meal_type_encoded, \n",
    "            diet_encoded\n",
    "        ])\n",
    "        \n",
    "        return X\n",
    "\n",
    "    bmi = calculate_bmi(user_profile['height'], user_profile['weight'])\n",
    "    bmi_category = categorize_bmi(bmi)\n",
    "    daily_calories = calculate_daily_calories(\n",
    "        user_profile['age'], \n",
    "        user_profile['gender'], \n",
    "        user_profile['weight'], \n",
    "        user_profile['height'], \n",
    "        user_profile['activity_level']\n",
    "    )\n",
    "\n",
    "    breakfast_calories = round(daily_calories * 0.25)\n",
    "    lunch_calories = round(daily_calories * 0.35)\n",
    "    dinner_calories = round(daily_calories * 0.30)\n",
    "    snack_calories = round(daily_calories * 0.10)\n",
    "\n",
    "    def recommend_menu(meal_type, cuisine_preference, calories):\n",
    "        min_calories = round(calories * 0.8)\n",
    "        max_calories = round(calories * 1.2)\n",
    "\n",
    "        available_cuisines = processed_df['Cuisine'].unique()\n",
    "        matched_cuisines = match_cuisine_preference(cuisine_preference, available_cuisines)\n",
    "\n",
    "        filtered_recipes = processed_df[\n",
    "            (processed_df['MealType'] == meal_type) &\n",
    "            (processed_df['Cuisine'].isin(matched_cuisines)) &\n",
    "            (processed_df['Calories'].between(min_calories, max_calories))\n",
    "        ]\n",
    "\n",
    "        if len(filtered_recipes) > 0:\n",
    "            recipe_features = prepare_recipe_features(filtered_recipes, model_data)\n",
    "            predicted_ratings = rating_prediction_model.predict(recipe_features).flatten()\n",
    "            \n",
    "            filtered_recipes['PredictedRating'] = predicted_ratings\n",
    "            recommended = filtered_recipes.sort_values('PredictedRating', ascending=False).head(3)\n",
    "        else:\n",
    "            recommended = filtered_recipes.head(3)\n",
    "\n",
    "        return recommended[['Name', 'Calories', 'ProteinContent', 'AggregatedRating', 'Cuisine']]\n",
    "\n",
    "    breakfast_menu = recommend_menu('Breakfast', user_profile['cuisine_preference'], breakfast_calories)\n",
    "    lunch_menu = recommend_menu('Lunch', user_profile['cuisine_preference'], lunch_calories)\n",
    "    dinner_menu = recommend_menu('Dinner', user_profile['cuisine_preference'], dinner_calories)\n",
    "\n",
    "    return {\n",
    "        'BMI': {\n",
    "            'value': bmi,\n",
    "            'category': bmi_category\n",
    "        },\n",
    "        'Daily Calories': {\n",
    "            'total': daily_calories,\n",
    "            'breakdown': {\n",
    "                'Breakfast': breakfast_calories,\n",
    "                'Lunch': lunch_calories,\n",
    "                'Dinner': dinner_calories,\n",
    "                'Snack': snack_calories\n",
    "            }\n",
    "        },\n",
    "        'Recommended Menus': {\n",
    "            'Breakfast': breakfast_menu,\n",
    "            'Lunch': lunch_menu,\n",
    "            'Dinner': dinner_menu\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c89f6d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "processed_df = preprocess_recipe_data(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdbb202a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From d:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2836/2836 [==============================] - 11s 3ms/step - loss: 1.3500 - mae: 0.8408 - val_loss: 0.4835 - val_mae: 0.5704\n",
      "Epoch 2/10\n",
      "2836/2836 [==============================] - 8s 3ms/step - loss: 0.6330 - mae: 0.5942 - val_loss: 0.4312 - val_mae: 0.4993\n",
      "Epoch 3/10\n",
      "2836/2836 [==============================] - 9s 3ms/step - loss: 0.5353 - mae: 0.5399 - val_loss: 0.4305 - val_mae: 0.4753\n",
      "Epoch 4/10\n",
      "2836/2836 [==============================] - 9s 3ms/step - loss: 0.4827 - mae: 0.5125 - val_loss: 0.4287 - val_mae: 0.4845\n",
      "Epoch 5/10\n",
      "2836/2836 [==============================] - 8s 3ms/step - loss: 0.4542 - mae: 0.5000 - val_loss: 0.4330 - val_mae: 0.5044\n",
      "Epoch 6/10\n",
      "2836/2836 [==============================] - 9s 3ms/step - loss: 0.4407 - mae: 0.4950 - val_loss: 0.4291 - val_mae: 0.4926\n",
      "Epoch 7/10\n",
      "2836/2836 [==============================] - 8s 3ms/step - loss: 0.4324 - mae: 0.4907 - val_loss: 0.4287 - val_mae: 0.4923\n",
      "Epoch 8/10\n",
      "2836/2836 [==============================] - 9s 3ms/step - loss: 0.4282 - mae: 0.4882 - val_loss: 0.4315 - val_mae: 0.5014\n",
      "Epoch 9/10\n",
      "2836/2836 [==============================] - 9s 3ms/step - loss: 0.4260 - mae: 0.4873 - val_loss: 0.4280 - val_mae: 0.4811\n",
      "Epoch 10/10\n",
      "2836/2836 [==============================] - 9s 3ms/step - loss: 0.4246 - mae: 0.4861 - val_loss: 0.4280 - val_mae: 0.4865\n",
      "887/887 [==============================] - 2s 2ms/step\n",
      "Test MAE: 0.47794671372757264\n",
      "Test MSE: 0.40300518741070146\n"
     ]
    }
   ],
   "source": [
    "# Persiapan Model\n",
    "model_data = prepare_model_data(processed_df)\n",
    "\n",
    "# Training Model\n",
    "rating_prediction_model, _ = train_rating_prediction_model(model_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "552b1b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:TensorFlow Decision Forests 1.8.1 is compatible with the following TensorFlow Versions: ['2.15.0']. However, TensorFlow 2.15.1 was detected. This can cause issues with the TF API and symbols in the custom C++ ops. See the TF and TF-DF compatibility table at https://github.com/tensorflow/decision-forests/blob/main/documentation/known_issues.md#compatibility-table.\n",
      "WARNING:root:Failure to load the inference.so custom c++ tensorflow ops. This error is likely caused the version of TensorFlow and TensorFlow Decision Forests are not compatible. Full error:d:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so not found\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "d:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m inference_so_dir \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpathsep \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Import TensorFlow.js module for saving the model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtfjs\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Function to save the trained model to TensorFlow.js\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_model_to_tfjs\u001b[39m(model, model_path):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Save the Keras model in TFJS format\u001b[39;00m\n",
      "File \u001b[1;32md:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\tensorflowjs\\__init__.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-imports\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m converters\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantization\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n",
      "File \u001b[1;32md:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\tensorflowjs\\converters\\__init__.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-imports,line-too-long\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_h5_conversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m save_keras_model\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tfjs_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deserialize_keras_model\n",
      "File \u001b[1;32md:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\tensorflowjs\\converters\\converter.py:38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_h5_conversion \u001b[38;5;28;01mas\u001b[39;00m conversion\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_tfjs_loader\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_saved_model_conversion_v2\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mzipfile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ZipFile, is_zipfile\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdispatch_keras_h5_to_tfjs_layers_model_conversion\u001b[39m(\n\u001b[0;32m     43\u001b[0m     h5_path, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, quantization_dtype_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m     split_weights_by_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     45\u001b[0m     weight_shard_size_bytes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     46\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[1;32md:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\tensorflowjs\\converters\\tf_saved_model_conversion_v2.py:28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mzipfile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ZipFile\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Required to load saved models that use TFDF.\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n",
      "File \u001b[1;32md:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\tensorflow_decision_forests\\__init__.py:64\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_version\n\u001b[0;32m     62\u001b[0m check_version\u001b[38;5;241m.\u001b[39mcheck_version(__version__, compatible_tf_versions)\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m py_tree\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m builder\n",
      "File \u001b[1;32md:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\tensorflow_decision_forests\\keras\\__init__.py:53\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Decision Forest in a Keras Model.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mUsage example:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callable, List\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wrappers\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Utility classes\u001b[39;00m\n",
      "File \u001b[1;32md:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\tensorflow_decision_forests\\keras\\core.py:62\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspector\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m inspector \u001b[38;5;28;01mas\u001b[39;00m inspector_lib\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tuner \u001b[38;5;28;01mas\u001b[39;00m tuner_lib\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m core_inference\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cc_logging\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m core \u001b[38;5;28;01mas\u001b[39;00m tf_core\n",
      "File \u001b[1;32md:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\tensorflow_decision_forests\\keras\\core_inference.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m core_inference \u001b[38;5;28;01mas\u001b[39;00m tf_core\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_logging\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m tf_op\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m abstract_learner_pb2\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultitasker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multitasker_pb2\n",
      "File \u001b[1;32md:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\api.py:179\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspector\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m inspector \u001b[38;5;28;01mas\u001b[39;00m inspector_lib\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf1_compatibility\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m op\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data_spec_pb2\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m abstract_model_pb2\n",
      "File \u001b[1;32md:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\op.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2021 Google LLC.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mop_dynamic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32md:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\op_dynamic.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     23\u001b[0m   check_version\u001b[38;5;241m.\u001b[39minfo_fail_to_load_custom_op(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference.so\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Importing all the symbols.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m]\n",
      "File \u001b[1;32md:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\op_dynamic.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 21\u001b[0m   ops \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_op_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path_to_datafile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minference.so\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     23\u001b[0m   check_version\u001b[38;5;241m.\u001b[39minfo_fail_to_load_custom_op(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference.so\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\load_library.py:54\u001b[0m, in \u001b[0;36mload_op_library\u001b[1;34m(library_filename)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_op_library\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_op_library\u001b[39m(library_filename):\n\u001b[0;32m     33\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a TensorFlow plugin, containing custom ops and kernels.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m  Pass \"library_filename\" to a platform-specific mechanism for dynamically\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    RuntimeError: when unable to load the library or get the python wrappers.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m   lib_handle \u001b[38;5;241m=\u001b[39m \u001b[43mpy_tf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_LoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlibrary_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     wrappers \u001b[38;5;241m=\u001b[39m _pywrap_python_op_gen\u001b[38;5;241m.\u001b[39mGetPythonWrappers(\n\u001b[0;32m     57\u001b[0m         py_tf\u001b[38;5;241m.\u001b[39mTF_GetOpList(lib_handle))\n",
      "\u001b[1;31mNotFoundError\u001b[0m: d:\\UB\\Dicoding\\GitHub\\Capstone_Akmal_Andri_Zahran\\Akmal\\.venv\\lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so not found"
     ]
    }
   ],
   "source": [
    "import os\n",
    "inference_so_dir = 'D:\\\\UB\\\\Dicoding\\\\GitHub\\\\Capstone_Akmal_Andri_Zahran\\\\Akmal\\\\.venv\\\\Lib\\\\site-packages\\\\tensorflow_decision_forests\\\\tensorflow\\\\ops\\\\inference'\n",
    "os.environ['PATH'] = inference_so_dir + os.pathsep + os.environ['PATH']\n",
    "\n",
    "# Import TensorFlow.js module for saving the model\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "# Function to save the trained model to TensorFlow.js\n",
    "def save_model_to_tfjs(model, model_path):\n",
    "    # Save the Keras model in TFJS format\n",
    "    tfjs.converters.save_keras_model(model, model_path)\n",
    "\n",
    "# Path where the model will be saved (you can specify your path)\n",
    "model_path = 'model_tfjs'\n",
    "\n",
    "# Assuming 'rating_prediction_model' is your trained model\n",
    "save_model_to_tfjs(rating_prediction_model, model_path)\n",
    "\n",
    "print(f'Model saved to TensorFlow.js format at: {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e2e301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rekomendasi untuk preferensi: Vegetarian, Asian\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akmal\\AppData\\Local\\Temp\\ipykernel_18932\\583768481.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_recipes['PredictedRating'] = predicted_ratings\n",
      "C:\\Users\\akmal\\AppData\\Local\\Temp\\ipykernel_18932\\583768481.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_recipes['PredictedRating'] = predicted_ratings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\n",
      "BMI:\n",
      "Nilai BMI: 34.6\n",
      "Kategori BMI: Obese\n",
      "\n",
      "Kebutuhan Kalori Harian:\n",
      "Total Kalori: 3940 kkal\n",
      "Pembagian Kalori:\n",
      "Breakfast: 985 kkal\n",
      "Lunch: 1379 kkal\n",
      "Dinner: 1182 kkal\n",
      "Snack: 394 kkal\n",
      "\n",
      "Rekomendasi Menu:\n",
      "\n",
      "Breakfast Menu:\n",
      "- Sticky Chilli Chicken Wings (Cuisine: Asian)\n",
      "  Kalori: 1009.0 kkal\n",
      "  Protein: 74.6 g\n",
      "  Rating: 5.0\n",
      "- Beef With Rice Noodles (Kway Teow) (Cuisine: Asian)\n",
      "  Kalori: 1106.8 kkal\n",
      "  Protein: 13.5 g\n",
      "  Rating: 5.0\n",
      "- Mom's Orange Curry Chicken (Cuisine: Asian)\n",
      "  Kalori: 794.0 kkal\n",
      "  Protein: 54.4 g\n",
      "  Rating: 5.0\n",
      "\n",
      "Lunch Menu:\n",
      "- Thai Beef Salad (Cuisine: Asian)\n",
      "  Kalori: 1259.9 kkal\n",
      "  Protein: 18.2 g\n",
      "  Rating: 4.5\n",
      "- Thai beef salad (Cuisine: Asian)\n",
      "  Kalori: 1490.3 kkal\n",
      "  Protein: 82.2 g\n",
      "  Rating: 5.0\n",
      "- Persian Lamb or Beef  Pumpkin Stew (Cuisine: Asian)\n",
      "  Kalori: 1413.7 kkal\n",
      "  Protein: 87.4 g\n",
      "  Rating: 5.0\n",
      "\n",
      "Dinner Menu:\n",
      "- Gluten Free Crunchy Chinese Noodles (Cuisine: Asian)\n",
      "  Kalori: 1126.5 kkal\n",
      "  Protein: 3.9 g\n",
      "  Rating: 5.0\n",
      "- CheK Chien (Fried Banana Nuggets) (Cuisine: Asian)\n",
      "  Kalori: 1111.1 kkal\n",
      "  Protein: 3.0 g\n",
      "  Rating: 5.0\n",
      "- Biryani (Cuisine: Asian)\n",
      "  Kalori: 1110.7 kkal\n",
      "  Protein: 63.4 g\n",
      "  Rating: 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akmal\\AppData\\Local\\Temp\\ipykernel_18932\\583768481.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_recipes['PredictedRating'] = predicted_ratings\n"
     ]
    }
   ],
   "source": [
    "# Contoh Profil Pengguna\n",
    "user_profiles = [\n",
    "    {\n",
    "        'age': 30,\n",
    "        'gender': 'male',\n",
    "        'height': 170,\n",
    "        'weight': 100,\n",
    "        'activity_level': 'very active',\n",
    "        'cuisine_preference': 'Vegetarian, Asian'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Rekomendasi untuk setiap profil\n",
    "for profile in user_profiles:\n",
    "    print(f\"\\nRekomendasi untuk preferensi: {profile['cuisine_preference']}\")\n",
    "    recommendation = recommend_recipe_comprehensive(\n",
    "        profile, \n",
    "        processed_df, \n",
    "        model_data, \n",
    "        rating_prediction_model\n",
    "    )\n",
    "    \n",
    "    # Cetak informasi BMI dan Kalori\n",
    "    print(\"\\nBMI:\")\n",
    "    print(f\"Nilai BMI: {recommendation['BMI']['value']}\")\n",
    "    print(f\"Kategori BMI: {recommendation['BMI']['category']}\")\n",
    "    \n",
    "    print(\"\\nKebutuhan Kalori Harian:\")\n",
    "    print(f\"Total Kalori: {recommendation['Daily Calories']['total']} kkal\")\n",
    "    print(\"Pembagian Kalori:\")\n",
    "    for meal, calories in recommendation['Daily Calories']['breakdown'].items():\n",
    "        print(f\"{meal}: {calories} kkal\")\n",
    "    \n",
    "    print(\"\\nRekomendasi Menu:\")\n",
    "    for meal_type, menu in recommendation['Recommended Menus'].items():\n",
    "        print(f\"\\n{meal_type} Menu:\")\n",
    "        for recipe in menu.to_dict('records'):\n",
    "            print(f\"- {recipe['Name']} (Cuisine: {recipe['Cuisine']})\")\n",
    "            print(f\"  Kalori: {recipe['Calories']} kkal\")\n",
    "            print(f\"  Protein: {recipe['ProteinContent']} g\")\n",
    "            print(f\"  Rating: {recipe['AggregatedRating']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
